{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision import models"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T02:48:27.561451300Z",
     "start_time": "2025-03-06T03:00:32.966046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_dir = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Dataset\\Dataset-used\"\n",
    "train_dir = \"C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Dataset\\\\Dataset-used\\\\train\"\n",
    "val_dir = \"C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Dataset\\\\Dataset-used\\\\valid\"\n",
    "test_dir = \"C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Dataset\\\\Dataset-used\\\\test\""
   ],
   "id": "f7932c516018943d",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T03:00:34.253054Z",
     "start_time": "2025-03-06T03:00:34.242956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DeepTreeModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DeepTreeModule, self).__init__()\n",
    "        # Nhánh 1: Sử dụng kernel size 3 (tập trung vào texture)\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        # Nhánh 2: Sử dụng kernel size 5 (tập trung vào hình dạng)\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        # Hợp nhất đầu ra của các nhánh\n",
    "        self.fc = nn.Linear(out_channels * 2, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, H, W]\n",
    "        b1 = self.branch1(x)  # [B, out_channels, 1, 1]\n",
    "        b2 = self.branch2(x)  # [B, out_channels, 1, 1]\n",
    "        # Flatten các đầu ra\n",
    "        b1 = b1.view(x.size(0), -1)\n",
    "        b2 = b2.view(x.size(0), -1)\n",
    "        # Nối chập theo chiều kênh\n",
    "        combined = torch.cat([b1, b2], dim=1)\n",
    "        out = self.fc(combined)\n",
    "        return out"
   ],
   "id": "4ef9c1f6905d9669",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T03:00:34.954847Z",
     "start_time": "2025-03-06T03:00:34.942233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PersonalizationModule(nn.Module):\n",
    "    def __init__(self, feature_dim, personalized_dim):\n",
    "        super(PersonalizationModule, self).__init__()\n",
    "        # Một mạng fully-connected đơn giản để điều chỉnh đặc trưng\n",
    "        self.fc1 = nn.Linear(feature_dim, personalized_dim)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(personalized_dim, feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x  # lưu lại đặc trưng ban đầu\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        # Skip connection giúp giữ lại đặc trưng gốc\n",
    "        out = out + identity\n",
    "        return out"
   ],
   "id": "3a89e7f51ebb173a",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T03:00:35.616254Z",
     "start_time": "2025-03-06T03:00:35.593609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FaceAntiSpoofingModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(FaceAntiSpoofingModel, self).__init__()\n",
    "        # Sử dụng MobileNetV3 làm backbone (pretrained)\n",
    "        mobilenet = models.mobilenet_v3_small(pretrained=True)\n",
    "        # Lấy phần features của MobileNetV3 (loại bỏ classifier)\n",
    "        self.backbone = mobilenet.features\n",
    "        # Kích thước đầu ra của MobileNetV3_small thường là 576 channels\n",
    "        backbone_out_channels = 576\n",
    "\n",
    "        # Module Deep Tree Learning\n",
    "        deep_tree_out_channels = 256  # có thể tùy chỉnh\n",
    "        self.deep_tree = DeepTreeModule(in_channels=backbone_out_channels,\n",
    "                                        out_channels=deep_tree_out_channels)\n",
    "\n",
    "        # Module cá nhân hóa: từ đặc trưng deep tree đến không gian cá nhân hóa\n",
    "        personalized_dim = 128\n",
    "        self.personalization = PersonalizationModule(feature_dim=deep_tree_out_channels,\n",
    "                                                     personalized_dim=personalized_dim)\n",
    "\n",
    "        # Lớp phân loại cuối cùng\n",
    "        self.classifier = nn.Linear(deep_tree_out_channels, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 3, H, W]\n",
    "        features = self.backbone(x)   # [B, 576, H', W']\n",
    "        tree_features = self.deep_tree(features)  # [B, 256]\n",
    "        # Áp dụng module cá nhân hóa\n",
    "        personalized_features = self.personalization(tree_features)  # [B, 256]\n",
    "        logits = self.classifier(personalized_features)  # [B, num_classes]\n",
    "        return logits"
   ],
   "id": "2749f03b68d37bd3",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T03:00:38.206330Z",
     "start_time": "2025-03-06T03:00:38.188174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        :param root_dir: Đường dẫn đến thư mục gốc (vd: train_dir hoặc val_dir).\n",
    "        :param transform: Các hàm tiền xử lý ảnh (Resize, Normalize, Augmentation, etc.)\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Lần lượt duyệt qua các lớp (folder fake và real)\n",
    "        for label, folder_name in enumerate(['fake', 'real']):  # 0: fake, 1: real\n",
    "            folder_path = os.path.join(root_dir, folder_name)\n",
    "            if not os.path.exists(folder_path):\n",
    "                print(f\"Warning: Folder {folder_path} not found!\")\n",
    "                continue\n",
    "\n",
    "            # Lấy danh sách các ảnh trong từng thư mục\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                self.data.append(file_path)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Đọc ảnh từ đường dẫn lưu trữ\n",
    "        img_path = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Mở và xử lý ảnh\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n"
   ],
   "id": "6cd9cc54ca5cd1cb",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T10:10:04.177147Z",
     "start_time": "2025-03-06T10:10:04.015951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Bộ tiền xử lý ảnh (resize, normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize ảnh về kích thước phù hợp\n",
    "    transforms.ToTensor(),  # Chuyển đổi thành dạng tensor\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Định nghĩa tập train_convert và val\n",
    "train_dataset = FaceDataset(root_dir=train_dir, transform=transform)\n",
    "val_dataset = FaceDataset(root_dir=val_dir, transform=transform)\n",
    "\n",
    "# Tạo DataLoader cho tập train_convert và val\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ],
   "id": "564ed37e031ecdb7",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T10:10:05.603423Z",
     "start_time": "2025-03-06T10:10:04.919548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tải mô hình đã huấn luyện\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FaceAntiSpoofingModel(num_classes=2)\n",
    "model.load_state_dict(torch.load(\"C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Model\\\\face_antispoofing_model.pth\", map_location=device))\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# Tạo một input mẫu (batch_size=1, 3 kênh, 224x224)\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Model\\\\face_antispoofing.onnx\",\n",
    "    export_params=True,\n",
    "    opset_version=11,\n",
    "    do_constant_folding=True,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"]\n",
    "    # Bỏ dynamic_axes để không sinh node \"Shape\"\n",
    ")\n",
    "print(\"Mô hình đã được chuyển đổi sang ONNX và lưu tại 'face_antispoofing.onnx'\")"
   ],
   "id": "f454376909ab4db5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mô hình đã được chuyển đổi sang ONNX và lưu tại 'face_antispoofing.onnx'\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T05:41:45.018144Z",
     "start_time": "2025-03-05T05:41:35.525294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import onnxruntime\n",
    "def calculate_acer_onnx(onnx_session, test_loader, device):\n",
    "    \"\"\"\n",
    "    Tính toán ACER, APCER, và BPCER dùng mô hình ONNX\n",
    "    :param onnx_session: Inference Session của ONNX Runtime\n",
    "    :param test_loader: DataLoader cho tập test\n",
    "    :param device: \"cpu\" hoặc \"cuda\" (GPU)\n",
    "    :return: ACER, APCER, BPCER\n",
    "    \"\"\"\n",
    "    total_real = 0\n",
    "    total_fake = 0\n",
    "    misclassified_real = 0\n",
    "    misclassified_fake = 0\n",
    "\n",
    "    # Đặt mô hình sang chế độ đánh giá\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            # Chuyển dữ liệu sang đúng device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Chuyển tensor images sang numpy vì ONNX yêu cầu đầu vào kiểu numpy\n",
    "            numpy_images = images.cpu().numpy()\n",
    "\n",
    "            # Chạy inferencing với mô hình ONNX\n",
    "            output = onnx_session.run(None, {onnx_session.get_inputs()[0].name: numpy_images})\n",
    "            predictions = np.argmax(output[0], axis=1)\n",
    "\n",
    "            # Phân loại các trường hợp\n",
    "            for label, prediction in zip(labels.cpu().numpy(), predictions):\n",
    "                if label == 0:\n",
    "                    total_fake += 1\n",
    "                    if prediction != 0:\n",
    "                        misclassified_fake += 1\n",
    "                elif label == 1:\n",
    "                    total_real += 1\n",
    "                    if prediction != 1:\n",
    "                        misclassified_real += 1\n",
    "\n",
    "    # Tính APCER và BPCER\n",
    "    apcer = misclassified_fake / total_fake if total_fake > 0 else 0\n",
    "    bpcer = misclassified_real / total_real if total_real > 0 else 0\n",
    "\n",
    "    # Tính ACER\n",
    "    acer = (apcer + bpcer) / 2\n",
    "\n",
    "    return acer, apcer, bpcer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "onnx_session = onnxruntime.InferenceSession(\"C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Model\\\\face_antispoofing.onnx\",\n",
    "                                            providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "acer, apcer, bpcer = calculate_acer_onnx(onnx_session, test_loader, device)\n",
    "\n",
    "print(f\"ACER: {acer:.4f}\")\n",
    "print(f\"APCER: {apcer:.4f}\")\n",
    "print(f\"BPCER: {bpcer:.4f}\")\n"
   ],
   "id": "92f582fe84782288",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACER: 0.0221\n",
      "APCER: 0.0112\n",
      "BPCER: 0.0329\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " ## Sử dụng Dataset cho representative dataset",
   "id": "df8249fce5aba297"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Pipeline này chỉ resize, chuyển đổi sang tensor và chuyển về dải giá trị [0,255].\n",
    "# Không có bước Normalize nào được áp dụng.\n",
    "representative_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),       # Resize ảnh về kích thước 224x224\n",
    "    transforms.ToTensor(),               # Chuyển đổi ảnh sang tensor với giá trị [0,1]\n",
    "    transforms.Lambda(lambda x: x * 255)   # Nhân với 255 để có giá trị pixel [0,255]\n",
    "])"
   ],
   "id": "ee7f78dde6660da0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T02:43:22.526243400Z",
     "start_time": "2025-03-06T02:31:50.585405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Giả sử đây là đường dẫn đến representative dataset (chứa thư mục 'fake' và 'real')\n",
    "representative_dir = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Dataset\\train_convert_reduced\"\n",
    "\n",
    "# Tạo dataset sử dụng representative_transform (không Normalize)\n",
    "rep_dataset = FaceDataset(root_dir=representative_dir, transform=representative_transform)\n",
    "\n",
    "# Ví dụ: in ra 2 mẫu để kiểm tra\n",
    "for i in range(2):\n",
    "    img, label = rep_dataset[i]\n",
    "    print(f\"Mẫu {i} - Label: {label}, Tensor shape: {img.shape}, min: {img.min()}, max: {img.max()}\")"
   ],
   "id": "aeb9da24630e3d0d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mẫu 0 - Label: 0, Tensor shape: torch.Size([3, 224, 224]), min: 0.0, max: 255.0\n",
      "Mẫu 1 - Label: 0, Tensor shape: torch.Size([3, 224, 224]), min: 0.0, max: 255.0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T02:42:31.757472Z",
     "start_time": "2025-03-06T02:42:14.071802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "# Thêm thư mục cha chứa folder 'onnx2tflite'\n",
    "sys.path.append(r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\onnx2tflite\")\n",
    "from onnx2tflite.converter import onnx_converter\n",
    "\n",
    "onnx_converter(\n",
    "    onnx_model_path = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Model\\face_antispoofing.onnx\",\n",
    "    need_simplify = True,\n",
    "    output_path = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Model\",\n",
    "    target_formats = ['keras', 'tflite'],  # or ['keras'], ['keras', 'tflite']\n",
    "    weight_quant = False,\n",
    "    int8_model = True,  # do quantification\n",
    "    int8_mean = [0.0, 0.0, 0.0],  # representative dataset raw, không normalize\n",
    "    int8_std = [1.0, 1.0, 1.0],\n",
    "    image_root = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Dataset\\train_convert_reduced\"\n",
    ")"
   ],
   "id": "2cd723d9386a1374",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 0/1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Quantization DataLoder ::Found 100 images: 50 fake, 50 real\n",
      "INFO:Quantization DataLoder ::Sample fake images: ['C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Dataset\\\\train_convert_reduced\\\\fake\\\\axonpp40_monitor_42012.jpg', 'C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Dataset\\\\train_convert_reduced\\\\fake\\\\cati2_phone_148_face.jpg', 'C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Dataset\\\\train_convert_reduced\\\\fake\\\\cati2_phone_38_face.jpg', 'C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Dataset\\\\train_convert_reduced\\\\fake\\\\cati2_phone_531_face.jpg', 'C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Dataset\\\\train_convert_reduced\\\\fake\\\\kagg19_phone_114.jpg']\n",
      "INFO:Quantization DataLoder ::Sample real images: ['C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Dataset\\\\train_convert_reduced\\\\real\\\\cati2_real_132.jpg', 'C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Dataset\\\\train_convert_reduced\\\\real\\\\cati2_real_193.jpg', 'C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Dataset\\\\train_convert_reduced\\\\real\\\\cati2_real_264.jpg', 'C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Dataset\\\\train_convert_reduced\\\\real\\\\cati2_real_639.jpg', 'C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Dataset\\\\train_convert_reduced\\\\real\\\\cati_real_ipcam_372.jpg']\n",
      "INFO:Quantization DataLoder ::100 images detected.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 54). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\GIGABYTE\\AppData\\Local\\Temp\\tmpd3tywg_g\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\GIGABYTE\\AppData\\Local\\Temp\\tmpd3tywg_g\\assets\n",
      "C:\\Users\\GIGABYTE\\WeatherPrediction\\.venv\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:789: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'keras': None,\n",
       " 'tflite': 'C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Model\\\\face_antispoofing_int8.tflite',\n",
       " 'keras_error': 0,\n",
       " 'tflite_error': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T02:53:43.094757Z",
     "start_time": "2025-03-06T02:53:31.136262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "# Thêm thư mục cha chứa folder 'onnx2tflite'\n",
    "sys.path.append(r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\onnx2tflite\")\n",
    "from onnx2tflite.converter import onnx_converter\n",
    "onnx_converter(\n",
    "    onnx_model_path = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Model\\face_antispoofing.onnx\",\n",
    "    need_simplify = True,\n",
    "    output_path = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Model\",\n",
    "    target_formats = ['tflite'], # or ['keras'], ['keras', 'tflite']\n",
    "    weight_quant = False,\n",
    "    fp16_model=False,\n",
    "    int8_model = False,\n",
    "    int8_mean = None,\n",
    "    int8_std = None,\n",
    "    image_root = None\n",
    ")"
   ],
   "id": "9fb6fc6032c1a217",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 0/1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 54). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\GIGABYTE\\AppData\\Local\\Temp\\tmpflq3su_s\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\GIGABYTE\\AppData\\Local\\Temp\\tmpflq3su_s\\assets\n",
      "WARNING:converter running::tflite model elements' max error is 3.7766E-04, pass, tflite saved in C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Model\\face_antispoofing.tflite\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'keras': None,\n",
       " 'tflite': 'C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Model\\\\face_antispoofing.tflite',\n",
       " 'keras_error': None,\n",
       " 'tflite_error': 0.00037765503}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T02:42:51.303055Z",
     "start_time": "2025-03-06T02:42:51.275381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Khởi tạo interpreter với mô hình TFLite\n",
    "tflite_model_path = \"C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Model\\\\face_antispoofing_int8.tflite\"\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Lấy thông tin input\n",
    "input_details = interpreter.get_input_details()\n",
    "print(\"Thông tin đầu vào:\", input_details)\n",
    "\n",
    "# Kiểm tra scale và zero_point của input\n",
    "input_scale, input_zero_point = input_details[0]['quantization']\n",
    "print(\"Input scale:\", input_scale)\n",
    "print(\"Input zero_point:\", input_zero_point)\n",
    "\n",
    "# Nếu cần, bạn cũng có thể kiểm tra thông tin output tương tự\n",
    "output_details = interpreter.get_output_details()\n",
    "print(\"Thông tin đầu ra:\", output_details)\n"
   ],
   "id": "99c559b32fe68ea8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thông tin đầu vào: [{'name': 'serving_default_input_2:0', 'index': 0, 'shape': array([  1, 224, 224,   3]), 'shape_signature': array([  1, 224, 224,   3]), 'dtype': <class 'numpy.uint8'>, 'quantization': (1.0, 0), 'quantization_parameters': {'scales': array([1.], dtype=float32), 'zero_points': array([0]), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Input scale: 1.0\n",
      "Input zero_point: 0\n",
      "Thông tin đầu ra: [{'name': 'StatefulPartitionedCall:0', 'index': 360, 'shape': array([1, 2]), 'shape_signature': array([1, 2]), 'dtype': <class 'numpy.uint8'>, 'quantization': (468.8677062988281, 122), 'quantization_parameters': {'scales': array([468.8677], dtype=float32), 'zero_points': array([122]), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T02:42:58.214047Z",
     "start_time": "2025-03-06T02:42:56.666909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Đường dẫn đến ảnh test, ví dụ:\n",
    "test_image_path = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Dataset\\train_convert\\real\\cati2_real_132.jpg\"\n",
    "test_img = Image.open(test_image_path).convert('RGB')\n",
    "test_img = representative_transform(test_img)\n",
    "\n",
    "# Chuyển đổi từ tensor (C, H, W) sang numpy định dạng NHWC (1, H, W, C)\n",
    "test_np = test_img.numpy()                # shape: (3, 224, 224)\n",
    "test_np = np.transpose(test_np, (1, 2, 0))  # shape: (224, 224, 3)\n",
    "test_np = np.expand_dims(test_np, axis=0)   # shape: (1, 224, 224, 3)\n",
    "test_np = test_np.astype(np.uint8)          # đảm bảo kiểu dữ liệu uint8\n",
    "\n",
    "# Tạo TFLite Interpreter và chạy inference\n",
    "tflite_model_path = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Model\\face_antispoofing_int8.tflite\"\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], test_np)\n",
    "interpreter.invoke()\n",
    "output = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "prediction = np.argmax(output, axis=1)[0]\n",
    "print(\"Prediction:\", prediction)\n",
    "print(\"Output raw:\", output)\n"
   ],
   "id": "4e163f6869074617",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0\n",
      "Output raw: [[122 122]]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### check tflite",
   "id": "bf8b9276be2c9d60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T02:55:08.735350Z",
     "start_time": "2025-03-06T02:55:08.674354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Định nghĩa pipeline inference: có thể sử dụng Normalize nếu mô hình gốc đã được huấn luyện với Normalize\n",
    "transform_inference = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Đường dẫn đến ảnh test\n",
    "image_path = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Dataset\\train_convert_reduced\\real\\cati2_real_132.jpg\"\n",
    "img = Image.open(image_path).convert('RGB')\n",
    "img_transformed = transform_inference(img)\n",
    "# Chuyển đổi tensor (C, H, W) sang numpy (1, H, W, C)\n",
    "img_np = img_transformed.numpy().transpose(1, 2, 0)\n",
    "img_np = np.expand_dims(img_np, axis=0).astype(np.float32)\n",
    "\n",
    "# Tạo TFLite Interpreter và chạy inference\n",
    "tflite_model_path = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Model\\face_antispoofing.tflite\"  # float model\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], img_np)\n",
    "interpreter.invoke()\n",
    "output = interpreter.get_tensor(output_details[0]['index'])\n",
    "prediction = np.argmax(output, axis=1)[0]\n",
    "\n",
    "print(\"Prediction:\", prediction)\n",
    "print(\"Output raw:\", output)\n"
   ],
   "id": "38c6cbdb6b309337",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1\n",
      "Output raw: [[-7.746914   7.3939495]]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T10:13:32.024597Z",
     "start_time": "2025-03-06T10:13:32.011504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Tạo test dataset và loader\n",
    "test_dataset = FaceDataset(root_dir=test_dir, transform=transform_inference)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ],
   "id": "cf0a8ca51e976552",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T03:01:35.717360Z",
     "start_time": "2025-03-06T03:01:16.578279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hàm tính toán các chỉ số từ mô hình TFLite\n",
    "def calculate_metrics_tflite(interpreter, test_loader):\n",
    "    \"\"\"\n",
    "    Tính toán ACER, APCER, BPCER từ mô hình TFLite float.\n",
    "    Giả sử mô hình có đầu vào shape: (batch, 224, 224, 3) và output shape: (batch, num_classes)\n",
    "    \"\"\"\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    total_real = 0\n",
    "    total_fake = 0\n",
    "    misclassified_real = 0\n",
    "    misclassified_fake = 0\n",
    "\n",
    "    # Lặp qua các batch trong test_loader\n",
    "    for images, labels in test_loader:\n",
    "        # images: tensor shape (batch, C, H, W)\n",
    "        batch = images.size(0)\n",
    "        # Chuyển sang numpy và chuyển từ NCHW sang NHWC\n",
    "        images_np = images.numpy().transpose(0, 2, 3, 1)  # shape: (batch, 224, 224, 3)\n",
    "\n",
    "        # Nếu mô hình TFLite hỗ trợ batch inference thì:\n",
    "        try:\n",
    "            interpreter.set_tensor(input_details[0]['index'], images_np)\n",
    "            interpreter.invoke()\n",
    "            outputs = interpreter.get_tensor(output_details[0]['index'])\n",
    "            # outputs: shape (batch, num_classes)\n",
    "            preds = np.argmax(outputs, axis=1)\n",
    "        except Exception as e:\n",
    "            # Nếu có lỗi với batch inference, chuyển sang inference từng ảnh\n",
    "            preds = []\n",
    "            for i in range(batch):\n",
    "                input_data = np.expand_dims(images_np[i], axis=0).astype(np.float32)\n",
    "                interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "                interpreter.invoke()\n",
    "                output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "                preds.append(np.argmax(output_data, axis=1)[0])\n",
    "            preds = np.array(preds)\n",
    "\n",
    "        # So sánh dự đoán với nhãn thật\n",
    "        for label, pred in zip(labels.numpy(), preds):\n",
    "            if label == 0:  # fake\n",
    "                total_fake += 1\n",
    "                if pred != 0:\n",
    "                    misclassified_fake += 1\n",
    "            elif label == 1:  # real\n",
    "                total_real += 1\n",
    "                if pred != 1:\n",
    "                    misclassified_real += 1\n",
    "\n",
    "    apcer = misclassified_fake / total_fake if total_fake > 0 else 0\n",
    "    bpcer = misclassified_real / total_real if total_real > 0 else 0\n",
    "    acer = (apcer + bpcer) / 2\n",
    "    return acer, apcer, bpcer\n",
    "\n",
    "# Đường dẫn mô hình TFLite float (chuyển đổi từ ONNX sang TFLite float, không quantized)\n",
    "tflite_model_path = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Model\\face_antispoofing.tflite\"\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Tính toán các chỉ số\n",
    "acer, apcer, bpcer = calculate_metrics_tflite(interpreter, test_loader)\n",
    "print(f\"ACER: {acer:.4f}\")\n",
    "print(f\"APCER: {apcer:.4f}\")\n",
    "print(f\"BPCER: {bpcer:.4f}\")"
   ],
   "id": "53b953b6c9ddb7d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACER: 0.0221\n",
      "APCER: 0.0112\n",
      "BPCER: 0.0329\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T03:04:42.629448Z",
     "start_time": "2025-03-06T03:04:42.615621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "def get_model_size(file_path):\n",
    "    size_bytes = os.path.getsize(file_path)\n",
    "    size_mb = size_bytes / (1024 * 1024)\n",
    "    return size_mb\n",
    "\n",
    "model_path = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Model\\face_antispoofing.tflite\"\n",
    "size = get_model_size(model_path)\n",
    "print(f\"Kích thước mô hình: {size:.2f} MB\")\n"
   ],
   "id": "d258900b9c82ffa5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kích thước mô hình: 23.45 MB\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Chuyển đổi xuống dạng float16",
   "id": "13440390ee354798"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T10:12:13.432420Z",
     "start_time": "2025-03-06T10:12:01.457820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "# Thêm thư mục cha chứa folder 'onnx2tflite'\n",
    "sys.path.append(r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\onnx2tflite\")\n",
    "from onnx2tflite.converter import onnx_converter\n",
    "\n",
    "onnx_converter(\n",
    "    onnx_model_path = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Model\\face_antispoofing.onnx\",\n",
    "    need_simplify = True,\n",
    "    output_path = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Model\",\n",
    "    target_formats = ['tflite'],  # chuyển đổi sang TFLite\n",
    "    weight_quant = False,\n",
    "    int8_model = False,    # vô hiệu hóa int8\n",
    "    fp16_model = True,     # kích hoạt float16\n",
    "    # Không cần representative dataset cho float16\n",
    ")\n"
   ],
   "id": "4a91c35eb3e6b2be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 0/1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 54). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\GIGABYTE\\AppData\\Local\\Temp\\tmp1ubjlpi4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\GIGABYTE\\AppData\\Local\\Temp\\tmp1ubjlpi4\\assets\n",
      "ERROR:converter running::tflite model elements' max error has reached 9.8256E-01, but convert is done, please check C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Model\\face_antispoofing_fp16.tflite carefully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'keras': None,\n",
       " 'tflite': 'C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Model\\\\face_antispoofing_fp16.tflite',\n",
       " 'keras_error': None,\n",
       " 'tflite_error': 0.98255754}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### check float16",
   "id": "b3b51a5831310498"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T10:17:17.731374Z",
     "start_time": "2025-03-06T10:17:00.653850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hàm tính toán các chỉ số từ mô hình TFLite\n",
    "def calculate_metrics_float16(interpreter, test_loader):\n",
    "    \"\"\"\n",
    "    Tính toán ACER, APCER, BPCER từ mô hình TFLite float.\n",
    "    Giả sử mô hình có đầu vào shape: (batch, 224, 224, 3) và output shape: (batch, num_classes)\n",
    "    \"\"\"\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    total_real = 0\n",
    "    total_fake = 0\n",
    "    misclassified_real = 0\n",
    "    misclassified_fake = 0\n",
    "\n",
    "    # Lặp qua các batch trong test_loader\n",
    "    for images, labels in test_loader:\n",
    "        # images: tensor shape (batch, C, H, W)\n",
    "        batch = images.size(0)\n",
    "        # Chuyển sang numpy và chuyển từ NCHW sang NHWC\n",
    "        images_np = images.numpy().transpose(0, 2, 3, 1)  # shape: (batch, 224, 224, 3)\n",
    "\n",
    "        # Nếu mô hình TFLite hỗ trợ batch inference thì:\n",
    "        try:\n",
    "            interpreter.set_tensor(input_details[0]['index'], images_np)\n",
    "            interpreter.invoke()\n",
    "            outputs = interpreter.get_tensor(output_details[0]['index'])\n",
    "            # outputs: shape (batch, num_classes)\n",
    "            preds = np.argmax(outputs, axis=1)\n",
    "        except Exception as e:\n",
    "            # Nếu có lỗi với batch inference, chuyển sang inference từng ảnh\n",
    "            preds = []\n",
    "            for i in range(batch):\n",
    "                input_data = np.expand_dims(images_np[i], axis=0).astype(np.float32)\n",
    "                interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "                interpreter.invoke()\n",
    "                output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "                preds.append(np.argmax(output_data, axis=1)[0])\n",
    "            preds = np.array(preds)\n",
    "\n",
    "        # So sánh dự đoán với nhãn thật\n",
    "        for label, pred in zip(labels.numpy(), preds):\n",
    "            if label == 0:  # fake\n",
    "                total_fake += 1\n",
    "                if pred != 0:\n",
    "                    misclassified_fake += 1\n",
    "            elif label == 1:  # real\n",
    "                total_real += 1\n",
    "                if pred != 1:\n",
    "                    misclassified_real += 1\n",
    "\n",
    "    apcer = misclassified_fake / total_fake if total_fake > 0 else 0\n",
    "    bpcer = misclassified_real / total_real if total_real > 0 else 0\n",
    "    acer = (apcer + bpcer) / 2\n",
    "    return acer, apcer, bpcer\n",
    "\n",
    "# Đường dẫn mô hình TFLite float (chuyển đổi từ ONNX sang TFLite float, không quantized)\n",
    "float16_model_path = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Model\\face_antispoofing_fp16.tflite\"\n",
    "interpreter = tf.lite.Interpreter(model_path=float16_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Tính toán các chỉ số\n",
    "acer, apcer, bpcer = calculate_metrics_float16(interpreter, test_loader)\n",
    "print(f\"ACER: {acer:.4f}\")\n",
    "print(f\"APCER: {apcer:.4f}\")\n",
    "print(f\"BPCER: {bpcer:.4f}\")"
   ],
   "id": "507dcb307e4e9404",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACER: 0.0200\n",
      "APCER: 0.0112\n",
      "BPCER: 0.0288\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T10:17:21.807172Z",
     "start_time": "2025-03-06T10:17:21.791487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "def get_model_size(file_path):\n",
    "    size_bytes = os.path.getsize(file_path)\n",
    "    size_mb = size_bytes / (1024 * 1024)\n",
    "    return size_mb\n",
    "\n",
    "\n",
    "size = get_model_size(float16_model_path)\n",
    "print(f\"Kích thước mô hình: {size:.2f} MB\")\n"
   ],
   "id": "172e04049ddfb5f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kích thước mô hình: 11.76 MB\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5a9f8b4206568e18"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
