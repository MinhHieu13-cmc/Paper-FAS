{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T04:42:34.146404Z",
     "start_time": "2025-03-12T04:42:30.104820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision import models"
   ],
   "id": "10cd459bf7cf7402",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T04:42:34.178034Z",
     "start_time": "2025-03-12T04:42:34.146404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_dir = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Dataset\\Dataset-used\"\n",
    "train_dir = \"C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Dataset\\\\Dataset-used\\\\train\"\n",
    "val_dir = \"C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Dataset\\\\Dataset-used\\\\valid\"\n",
    "test_dir = \"C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Dataset\\\\Dataset-used\\\\test\""
   ],
   "id": "cd259863e706d771",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T04:42:36.487340Z",
     "start_time": "2025-03-12T04:42:36.471407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DeepTreeModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DeepTreeModule, self).__init__()\n",
    "        # Nh?nh 1: S? d?ng kernel size 3 (t?p trung v?o texture)\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        # Nh?nh 2: S? d?ng kernel size 5 (t?p trung v?o h?nh d?ng)\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        # H?p nh?t ??u ra c?a c?c nh?nh\n",
    "        self.fc = nn.Linear(out_channels * 2, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, H, W]\n",
    "        b1 = self.branch1(x)  # [B, out_channels, 1, 1]\n",
    "        b2 = self.branch2(x)  # [B, out_channels, 1, 1]\n",
    "        # Flatten c?c ??u ra\n",
    "        b1 = b1.view(x.size(0), -1)\n",
    "        b2 = b2.view(x.size(0), -1)\n",
    "        # N?i ch?p theo chi?u k?nh\n",
    "        combined = torch.cat([b1, b2], dim=1)\n",
    "        out = self.fc(combined)\n",
    "        return out"
   ],
   "id": "fa52de2aa679c84a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T04:42:36.878392Z",
     "start_time": "2025-03-12T04:42:36.862572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PersonalizationModule(nn.Module):\n",
    "    def __init__(self, feature_dim, personalized_dim):\n",
    "        super(PersonalizationModule, self).__init__()\n",
    "        # M?t m?ng fully-connected ??n gi?n ?? ?i?u ch?nh ??c tr?ng\n",
    "        self.fc1 = nn.Linear(feature_dim, personalized_dim)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(personalized_dim, feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x  # l?u l?i ??c tr?ng ban ??u\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        # Skip connection gi?p gi? l?i ??c tr?ng g?c\n",
    "        out = out + identity\n",
    "        return out"
   ],
   "id": "3d100d6b41ca033e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T04:42:37.323668Z",
     "start_time": "2025-03-12T04:42:37.292019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FaceAntiSpoofingModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(FaceAntiSpoofingModel, self).__init__()\n",
    "        # S? d?ng MobileNetV3 l?m backbone (pretrained)\n",
    "        mobilenet = models.mobilenet_v3_small(pretrained=True)\n",
    "        # L?y ph?n features c?a MobileNetV3 (lo?i b? classifier)\n",
    "        self.backbone = mobilenet.features\n",
    "        # K?ch th??c ??u ra c?a MobileNetV3_small th??ng l? 576 channels\n",
    "        backbone_out_channels = 576\n",
    "\n",
    "        # Module Deep Tree Learning\n",
    "        deep_tree_out_channels = 256  # c? th? t?y ch?nh\n",
    "        self.deep_tree = DeepTreeModule(in_channels=backbone_out_channels,\n",
    "                                        out_channels=deep_tree_out_channels)\n",
    "\n",
    "        # Module c? nh?n h?a: t? ??c tr?ng deep tree ??n kh?ng gian c? nh?n h?a\n",
    "        personalized_dim = 128\n",
    "        self.personalization = PersonalizationModule(feature_dim=deep_tree_out_channels,\n",
    "                                                     personalized_dim=personalized_dim)\n",
    "\n",
    "        # L?p ph?n lo?i cu?i c?ng\n",
    "        self.classifier = nn.Linear(deep_tree_out_channels, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 3, H, W]\n",
    "        features = self.backbone(x)   # [B, 576, H', W']\n",
    "        tree_features = self.deep_tree(features)  # [B, 256]\n",
    "        # ?p d?ng module c? nh?n h?a\n",
    "        personalized_features = self.personalization(tree_features)  # [B, 256]\n",
    "        logits = self.classifier(personalized_features)  # [B, num_classes]\n",
    "        return logits"
   ],
   "id": "1a7054a95e4d9010",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T04:42:39.529099Z",
     "start_time": "2025-03-12T04:42:39.513279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        :param root_dir: ???ng d?n ??n th? m?c g?c (vd: train_dir ho?c val_dir).\n",
    "        :param transform: C?c h?m ti?n x? l? ?nh (Resize, Normalize, Augmentation, etc.)\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        # L?n l??t duy?t qua c?c l?p (folder fake v? real)\n",
    "        for label, folder_name in enumerate(['fake', 'real']):  # 0: fake, 1: real\n",
    "            folder_path = os.path.join(root_dir, folder_name)\n",
    "            if not os.path.exists(folder_path):\n",
    "                print(f\"Warning: Folder {folder_path} not found!\")\n",
    "                continue\n",
    "\n",
    "            # L?y danh s?ch c?c ?nh trong t?ng th? m?c\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                self.data.append(file_path)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ??c ?nh t? ???ng d?n l?u tr?\n",
    "        img_path = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # M? v? x? l? ?nh\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n"
   ],
   "id": "3b2802e14398c2ef",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T03:27:19.445103Z",
     "start_time": "2025-03-11T03:27:18.601690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# B? ti?n x? l? ?nh (resize, normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize ?nh v? k?ch th??c ph? h?p\n",
    "    transforms.ToTensor(),  # Chuy?n ??i th?nh d?ng tensor\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ??nh ngh?a t?p train_convert v? val\n",
    "train_dataset = FaceDataset(root_dir=train_dir, transform=transform)\n",
    "val_dataset = FaceDataset(root_dir=val_dir, transform=transform)\n",
    "\n",
    "# T?o DataLoader cho t?p train_convert v? val\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ],
   "id": "85651bf9889ce353",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from FaceAntiSpoofingModel import FaceAntiSpoofingModel\n",
    "\n",
    "# Bước 1: Tạo instance của mô hình\n",
    "model_q = FaceAntiSpoofingModel(num_classes=2)\n",
    "\n",
    "# Bước 2: Tải state_dict đã lưu (mô hình đã lượng tử hóa)\n",
    "state_dict = torch.load(\"face_antispoofing_model_quantized.pth\", map_location=torch.device(\"cpu\"))\n",
    "model_q.load_state_dict(state_dict)\n",
    "\n",
    "# Bước 3: Chuyển mô hình sang chế độ eval và về CPU\n",
    "model_q.eval()\n",
    "model_q.to(\"cpu\")\n",
    "\n",
    "# Bước 4: Xuất sang ONNX\n",
    "dummy_input = torch.randn(1, 3, 224, 224)  # Tạo một input mẫu\n",
    "torch.onnx.export(\n",
    "    model_q,\n",
    "    dummy_input,\n",
    "    \"face_antispoofing_quantized.onnx\",\n",
    "    export_params=True,\n",
    "    opset_version=12,\n",
    "    do_constant_folding=True,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\n",
    ")\n",
    "\n",
    "print(\"Mô hình đã được chuyển đổi sang ONNX và lưu tại 'face_antispoofing_quantized.onnx'\")"
   ],
   "id": "3a37565062a02c98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import onnxruntime\n",
    "def calculate_acer_onnx(onnx_session, test_loader, device):\n",
    "    \"\"\"\n",
    "    T?nh to?n ACER, APCER, v? BPCER d?ng m? h?nh ONNX\n",
    "    :param onnx_session: Inference Session c?a ONNX Runtime\n",
    "    :param test_loader: DataLoader cho t?p test\n",
    "    :param device: \"cpu\" ho?c \"cuda\" (GPU)\n",
    "    :return: ACER, APCER, BPCER\n",
    "    \"\"\"\n",
    "    total_real = 0\n",
    "    total_fake = 0\n",
    "    misclassified_real = 0\n",
    "    misclassified_fake = 0\n",
    "\n",
    "    # ??t m? h?nh sang ch? ?? ??nh gi?\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            # Chuy?n d? li?u sang ??ng device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Chuy?n tensor images sang numpy v? ONNX y?u c?u ??u v?o ki?u numpy\n",
    "            numpy_images = images.cpu().numpy()\n",
    "\n",
    "            # Ch?y inferencing v?i m? h?nh ONNX\n",
    "            output = onnx_session.run(None, {onnx_session.get_inputs()[0].name: numpy_images})\n",
    "            predictions = np.argmax(output[0], axis=1)\n",
    "\n",
    "            # Ph?n lo?i c?c tr??ng h?p\n",
    "            for label, prediction in zip(labels.cpu().numpy(), predictions):\n",
    "                if label == 0:\n",
    "                    total_fake += 1\n",
    "                    if prediction != 0:\n",
    "                        misclassified_fake += 1\n",
    "                elif label == 1:\n",
    "                    total_real += 1\n",
    "                    if prediction != 1:\n",
    "                        misclassified_real += 1\n",
    "\n",
    "    # T?nh APCER v? BPCER\n",
    "    apcer = misclassified_fake / total_fake if total_fake > 0 else 0\n",
    "    bpcer = misclassified_real / total_real if total_real > 0 else 0\n",
    "\n",
    "    # T?nh ACER\n",
    "    acer = (apcer + bpcer) / 2\n",
    "\n",
    "    return acer, apcer, bpcer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "onnx_session = onnxruntime.InferenceSession(\"C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Model\\\\face_antispoofing.onnx\",\n",
    "                                            providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "acer, apcer, bpcer = calculate_acer_onnx(onnx_session, test_loader, device)\n",
    "\n",
    "print(f\"ACER: {acer:.4f}\")\n",
    "print(f\"APCER: {apcer:.4f}\")\n",
    "print(f\"BPCER: {bpcer:.4f}\")\n"
   ],
   "id": "17fc9d6628031ed6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " ## Su dung Dataset cho representative dataset",
   "id": "a72dde9f402ac980"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T10:32:05.738734Z",
     "start_time": "2025-03-12T10:31:34.602468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Định nghĩa pipeline augmentation cho representative dataset\n",
    "rep_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomResizedCrop(size=224, scale=(0.9, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x * 255)\n",
    "])\n",
    "\n",
    "# Đường dẫn representative dataset (có thể có các thư mục con như 'fake', 'real')\n",
    "representative_dir = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Dataset\\Dataset-used\\valid\"\n",
    "\n",
    "# Lấy danh sách tất cả các file ảnh\n",
    "image_paths = []\n",
    "for root, _, files in os.walk(representative_dir):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_paths.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Tổng số ảnh trong representative dataset: {len(image_paths)}\\n\")\n",
    "\n",
    "# Load tất cả ảnh và lưu vào danh sách\n",
    "all_images = []\n",
    "for img_path in image_paths:\n",
    "    try:\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        tensor = rep_transform(img)  # Kích thước: (3, 224, 224), giá trị [0,255]\n",
    "        all_images.append(tensor)\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi load {img_path}: {e}\")\n",
    "\n",
    "if len(all_images) == 0:\n",
    "    raise ValueError(\"Không load được ảnh hợp lệ từ representative dataset!\")\n",
    "\n",
    "# Stack các tensor thành một tensor lớn: (N, 3, 224, 224)\n",
    "all_images_tensor = torch.stack(all_images)\n",
    "\n",
    "# Tính toán mean và std theo từng kênh (RGB) trên toàn bộ dataset\n",
    "mean = torch.mean(all_images_tensor, dim=[0, 2, 3])\n",
    "std = torch.std(all_images_tensor, dim=[0, 2, 3])\n",
    "\n",
    "mean_np = mean.numpy().tolist()\n",
    "std_np = std.numpy().tolist()\n",
    "\n",
    "print(\"Mean (sau khi nhân 255):\", mean_np)\n",
    "print(\"Std (sau khi nhân 255):\", std_np)\n"
   ],
   "id": "d3243fc8fd6369d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng số ảnh trong representative dataset: 4226\n",
      "\n",
      "Mean (sau khi nhân 255): [140.64718627929688, 114.01417541503906, 103.77151489257812]\n",
      "Std (sau khi nhân 255): [62.949188232421875, 57.36742401123047, 53.96285629272461]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T12:29:22.194352Z",
     "start_time": "2025-03-12T12:21:00.484003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "# Thêm đường dẫn đến thư mục onnx2tflite (nếu chưa cài đặt qua pip)\n",
    "sys.path.append(r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\onnx2tflite\")\n",
    "\n",
    "from onnx2tflite.converter import onnx_converter\n",
    "\n",
    "onnx_converter(\n",
    "    onnx_model_path = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\model2\\face_antispoofing_v2.onnx\",\n",
    "    need_simplify = True,\n",
    "    output_path = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\model2\",\n",
    "    target_formats = 'tflite',  # chuyển sang TFLite\n",
    "    weight_quant = False,\n",
    "    int8_model = True,\n",
    "    int8_mean = mean_np,\n",
    "    int8_std = std_np,\n",
    "    image_root = representative_dir\n",
    ")"
   ],
   "id": "ca4a7257204537af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 0/1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Quantization DataLoder ::Found 4226 images: 2113 fake, 2113 real\n",
      "INFO:Quantization DataLoder ::Sample fake images: ['C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Dataset\\\\Dataset-used\\\\valid\\\\fake\\\\axon2p1219_silicon_54281.jpg', 'C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Dataset\\\\Dataset-used\\\\valid\\\\fake\\\\axon2p122_silicon_280198.jpg', 'C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Dataset\\\\Dataset-used\\\\valid\\\\fake\\\\axon2p1232_silicon_13695.jpg', 'C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Dataset\\\\Dataset-used\\\\valid\\\\fake\\\\axon2p1344_silicon_462893.jpg', 'C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Dataset\\\\Dataset-used\\\\valid\\\\fake\\\\axon2p2009_silicon_2342880.jpg']\n",
      "INFO:Quantization DataLoder ::Sample real images: ['C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Dataset\\\\Dataset-used\\\\valid\\\\real\\\\asian2_real_video_36.jpg', 'C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Dataset\\\\Dataset-used\\\\valid\\\\real\\\\asian3_real_video_1.jpg', 'C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Dataset\\\\Dataset-used\\\\valid\\\\real\\\\asian3_real_video_175.jpg', 'C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Dataset\\\\Dataset-used\\\\valid\\\\real\\\\asian3_real_video_35.jpg', 'C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\Dataset\\\\Dataset-used\\\\valid\\\\real\\\\asian4_real_video_1.jpg']\n",
      "WARNING:Quantization DataLoder ::4226 images detected, the number of recommended images is less than 100.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\GIGABYTE\\AppData\\Local\\Temp\\tmpq186bx38\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\GIGABYTE\\AppData\\Local\\Temp\\tmpq186bx38\\assets\n",
      "C:\\Users\\GIGABYTE\\WeatherPrediction\\.venv\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:887: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'keras': None,\n",
       " 'tflite': 'C:\\\\Users\\\\GIGABYTE\\\\PycharmProjects\\\\Paper-FAS\\\\model2\\\\face_antispoofing_v2_int8.tflite',\n",
       " 'keras_error': 0,\n",
       " 'tflite_error': 0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T03:33:49.916042Z",
     "start_time": "2025-03-13T03:33:40.737713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Pipeline inference (khớp với representative dataset)\n",
    "inf_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomResizedCrop(size=224, scale=(0.9, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x * 255)\n",
    "])\n",
    "\n",
    "# Đường dẫn file ảnh test\n",
    "test_image_path = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Dataset\\Dataset-used\\test\\real\\asian1_real_selfie.jpg\"\n",
    "\n",
    "# Load và xử lý ảnh test\n",
    "test_img = Image.open(test_image_path).convert('RGB')\n",
    "test_img = inf_transform(test_img)  # (3, 224, 224)\n",
    "test_np = test_img.numpy()           # (3, 224, 224)\n",
    "test_np = np.transpose(test_np, (1, 2, 0))  # (224, 224, 3)\n",
    "test_np = np.expand_dims(test_np, axis=0)     # (1, 224, 224, 3)\n",
    "test_np = test_np.astype(np.uint8)\n",
    "\n",
    "# Khởi tạo TFLite Interpreter và load mô hình\n",
    "tflite_model_path = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\model2\\face_antispoofing_v2_int8.tflite\"\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Lấy thông tin input và output\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "print(\"Input details:\", input_details)\n",
    "print(\"Output details:\", output_details)\n",
    "\n",
    "# Đưa dữ liệu vào mô hình và chạy inference\n",
    "interpreter.set_tensor(input_details[0]['index'], test_np)\n",
    "interpreter.invoke()\n",
    "output = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Dequantize output\n",
    "scale, zero_point = output_details[0]['quantization']\n",
    "output_float = (output.astype(np.float32) - zero_point) * scale if scale != 0 else output\n",
    "prediction = np.argmax(output_float, axis=1)[0]\n",
    "\n",
    "print(\"Prediction:\", prediction)\n",
    "print(\"Output raw (dequantized):\", output_float)"
   ],
   "id": "aadac94b92219dee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input details: [{'name': 'serving_default_input_5:0', 'index': 0, 'shape': array([  1, 224, 224,   3]), 'shape_signature': array([  1, 224, 224,   3]), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.01975196972489357, 113), 'quantization_parameters': {'scales': array([0.01975197], dtype=float32), 'zero_points': array([113]), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Output details: [{'name': 'StatefulPartitionedCall:0', 'index': 199, 'shape': array([1, 2]), 'shape_signature': array([1, 2]), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.18672429025173187, 127), 'quantization_parameters': {'scales': array([0.18672429], dtype=float32), 'zero_points': array([127]), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Prediction: 0\n",
      "Output raw (dequantized): [[ 2.61414 -2.61414]]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T06:39:45.995632Z",
     "start_time": "2025-03-12T06:39:45.971369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Giả sử interpreter đã được khởi tạo và allocate_tensors() được gọi\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# In thông tin chi tiết của input\n",
    "for i, inp in enumerate(input_details):\n",
    "    print(f\"Input {i}:\")\n",
    "    print(\"  Shape:\", inp['shape'])\n",
    "    print(\"  Dtype:\", inp['dtype'])\n",
    "    print(\"  Quantization:\", inp['quantization'])\n",
    "\n",
    "# In thông tin chi tiết của output\n",
    "for i, out in enumerate(output_details):\n",
    "    print(f\"Output {i}:\")\n",
    "    print(\"  Shape:\", out['shape'])\n",
    "    print(\"  Dtype:\", out['dtype'])\n",
    "    print(\"  Quantization:\", out['quantization'])\n"
   ],
   "id": "88bd97e0d3faed5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 0:\n",
      "  Shape: [  1 224 224   3]\n",
      "  Dtype: <class 'numpy.uint8'>\n",
      "  Quantization: (0.019774096086621284, 113)\n",
      "Output 0:\n",
      "  Shape: [1 2]\n",
      "  Dtype: <class 'numpy.uint8'>\n",
      "  Quantization: (0.2599039673805237, 137)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### check tflite",
   "id": "2731c477f6f47af4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T09:56:03.307879Z",
     "start_time": "2025-03-11T09:56:03.241191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# ??nh ngh?a pipeline inference: c? th? s? d?ng Normalize n?u m? h?nh g?c ?? ???c hu?n luy?n v?i Normalize\n",
    "transform_inference = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ???ng d?n ??n ?nh test\n",
    "image_path = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Dataset\\train_convert_reduced\\real\\cati2_real_132.jpg\"\n",
    "img = Image.open(image_path).convert('RGB')\n",
    "img_transformed = transform_inference(img)\n",
    "# Chuy?n ??i tensor (C, H, W) sang numpy (1, H, W, C)\n",
    "img_np = img_transformed.numpy().transpose(1, 2, 0)\n",
    "img_np = np.expand_dims(img_np, axis=0).astype(np.float32)\n",
    "\n",
    "# T?o TFLite Interpreter v? ch?y inference\n",
    "tflite_model_path = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Model\\face_antispoofing.tflite\"  # float model\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], img_np)\n",
    "interpreter.invoke()\n",
    "output = interpreter.get_tensor(output_details[0]['index'])\n",
    "prediction = np.argmax(output, axis=1)[0]\n",
    "\n",
    "print(\"Prediction:\", prediction)\n",
    "print(\"Output raw:\", output)\n"
   ],
   "id": "b2632c7f30805ab9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1\n",
      "Output raw: [[-7.746914   7.3939495]]\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T03:12:09.247536Z",
     "start_time": "2025-03-11T03:12:09.212693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# T?o test dataset v? loader\n",
    "test_dataset = FaceDataset(root_dir=test_dir, transform=transform_inference)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ],
   "id": "2af5ec93f121db12",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T03:12:32.142329Z",
     "start_time": "2025-03-11T03:12:14.018728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# H?m t?nh to?n c?c ch? s? t? m? h?nh TFLite\n",
    "def calculate_metrics_tflite(interpreter, test_loader):\n",
    "    \"\"\"\n",
    "    T?nh to?n ACER, APCER, BPCER t? m? h?nh TFLite float.\n",
    "    Gi? s? m? h?nh c? ??u v?o shape: (batch, 224, 224, 3) v? output shape: (batch, num_classes)\n",
    "    \"\"\"\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    total_real = 0\n",
    "    total_fake = 0\n",
    "    misclassified_real = 0\n",
    "    misclassified_fake = 0\n",
    "\n",
    "    # L?p qua c?c batch trong test_loader\n",
    "    for images, labels in test_loader:\n",
    "        # images: tensor shape (batch, C, H, W)\n",
    "        batch = images.size(0)\n",
    "        # Chuy?n sang numpy v? chuy?n t? NCHW sang NHWC\n",
    "        images_np = images.numpy().transpose(0, 2, 3, 1)  # shape: (batch, 224, 224, 3)\n",
    "\n",
    "        # N?u m? h?nh TFLite h? tr? batch inference th?:\n",
    "        try:\n",
    "            interpreter.set_tensor(input_details[0]['index'], images_np)\n",
    "            interpreter.invoke()\n",
    "            outputs = interpreter.get_tensor(output_details[0]['index'])\n",
    "            # outputs: shape (batch, num_classes)\n",
    "            preds = np.argmax(outputs, axis=1)\n",
    "        except Exception as e:\n",
    "            # N?u c? l?i v?i batch inference, chuy?n sang inference t?ng ?nh\n",
    "            preds = []\n",
    "            for i in range(batch):\n",
    "                input_data = np.expand_dims(images_np[i], axis=0).astype(np.float32)\n",
    "                interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "                interpreter.invoke()\n",
    "                output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "                preds.append(np.argmax(output_data, axis=1)[0])\n",
    "            preds = np.array(preds)\n",
    "\n",
    "        # So s?nh d? ?o?n v?i nh?n th?t\n",
    "        for label, pred in zip(labels.numpy(), preds):\n",
    "            if label == 0:  # fake\n",
    "                total_fake += 1\n",
    "                if pred != 0:\n",
    "                    misclassified_fake += 1\n",
    "            elif label == 1:  # real\n",
    "                total_real += 1\n",
    "                if pred != 1:\n",
    "                    misclassified_real += 1\n",
    "\n",
    "    apcer = misclassified_fake / total_fake if total_fake > 0 else 0\n",
    "    bpcer = misclassified_real / total_real if total_real > 0 else 0\n",
    "    acer = (apcer + bpcer) / 2\n",
    "    return acer, apcer, bpcer\n",
    "\n",
    "# ???ng d?n m? h?nh TFLite float (chuy?n ??i t? ONNX sang TFLite float, kh?ng quantized)\n",
    "tflite_model_path = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Model\\face_antispoofing.tflite\"\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# T?nh to?n c?c ch? s?\n",
    "acer, apcer, bpcer = calculate_metrics_tflite(interpreter, test_loader)\n",
    "print(f\"ACER: {acer:.4f}\")\n",
    "print(f\"APCER: {apcer:.4f}\")\n",
    "print(f\"BPCER: {bpcer:.4f}\")"
   ],
   "id": "68b9937697d05943",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACER: 0.0221\n",
      "APCER: 0.0112\n",
      "BPCER: 0.0329\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "def get_model_size(file_path):\n",
    "    size_bytes = os.path.getsize(file_path)\n",
    "    size_mb = size_bytes / (1024 * 1024)\n",
    "    return size_mb\n",
    "\n",
    "model_path = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Model\\face_antispoofing.tflite\"\n",
    "size = get_model_size(model_path)\n",
    "print(f\"K?ch th??c m? h?nh: {size:.2f} MB\")\n"
   ],
   "id": "26850dde93522897"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Chuy?n ??i xu?ng d?ng float16",
   "id": "d5d9579429d91071"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sys\n",
    "# Th?m th? m?c cha ch?a folder 'onnx2tflite'\n",
    "sys.path.append(r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\onnx2tflite\")\n",
    "from onnx2tflite.converter import onnx_converter\n",
    "\n",
    "onnx_converter(\n",
    "    onnx_model_path = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Model\\face_antispoofing.onnx\",\n",
    "    need_simplify = True,\n",
    "    output_path = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Model\",\n",
    "    target_formats = ['tflite'],  # chuy?n ??i sang TFLite\n",
    "    weight_quant = False,\n",
    "    int8_model = False,    # v? hi?u h?a int8\n",
    "    fp16_model = True,     # k?ch ho?t float16\n",
    ")\n"
   ],
   "id": "3a41a4b28947630e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T09:47:44.649992Z",
     "start_time": "2025-03-11T09:47:44.612150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Giả sử interpreter đã được khởi tạo và allocate_tensors() được gọi\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# In thông tin chi tiết của input\n",
    "for i, inp in enumerate(input_details):\n",
    "    print(f\"Input {i}:\")\n",
    "    print(\"  Shape:\", inp['shape'])\n",
    "    print(\"  Dtype:\", inp['dtype'])\n",
    "    print(\"  Quantization:\", inp['quantization'])\n",
    "\n",
    "# In thông tin chi tiết của output\n",
    "for i, out in enumerate(output_details):\n",
    "    print(f\"Output {i}:\")\n",
    "    print(\"  Shape:\", out['shape'])\n",
    "    print(\"  Dtype:\", out['dtype'])\n",
    "    print(\"  Quantization:\", out['quantization'])\n"
   ],
   "id": "63eecf6850df7b26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 0:\n",
      "  Shape: [  1 224 224   3]\n",
      "  Dtype: <class 'numpy.uint8'>\n",
      "  Quantization: (0.020829275250434875, 115)\n",
      "Output 0:\n",
      "  Shape: [1 2]\n",
      "  Dtype: <class 'numpy.uint8'>\n",
      "  Quantization: (0.6434805393218994, 122)\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### check float16",
   "id": "f4f1f3883def0820"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T03:24:06.744083Z",
     "start_time": "2025-03-11T03:23:49.606139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# H?m t?nh to?n c?c ch? s? t? m? h?nh TFLite\n",
    "def calculate_metrics_float16(interpreter, test_loader):\n",
    "    \"\"\"\n",
    "    T?nh to?n ACER, APCER, BPCER t? m? h?nh TFLite float.\n",
    "    Gi? s? m? h?nh c? ??u v?o shape: (batch, 224, 224, 3) v? output shape: (batch, num_classes)\n",
    "    \"\"\"\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    total_real = 0\n",
    "    total_fake = 0\n",
    "    misclassified_real = 0\n",
    "    misclassified_fake = 0\n",
    "\n",
    "    # L?p qua c?c batch trong test_loader\n",
    "    for images, labels in test_loader:\n",
    "        # images: tensor shape (batch, C, H, W)\n",
    "        batch = images.size(0)\n",
    "        # Chuy?n sang numpy v? chuy?n t? NCHW sang NHWC\n",
    "        images_np = images.numpy().transpose(0, 2, 3, 1)  # shape: (batch, 224, 224, 3)\n",
    "\n",
    "        # N?u m? h?nh TFLite h? tr? batch inference th?:\n",
    "        try:\n",
    "            interpreter.set_tensor(input_details[0]['index'], images_np)\n",
    "            interpreter.invoke()\n",
    "            outputs = interpreter.get_tensor(output_details[0]['index'])\n",
    "            # outputs: shape (batch, num_classes)\n",
    "            preds = np.argmax(outputs, axis=1)\n",
    "        except Exception as e:\n",
    "            # N?u c? l?i v?i batch inference, chuy?n sang inference t?ng ?nh\n",
    "            preds = []\n",
    "            for i in range(batch):\n",
    "                input_data = np.expand_dims(images_np[i], axis=0).astype(np.float32)\n",
    "                interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "                interpreter.invoke()\n",
    "                output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "                preds.append(np.argmax(output_data, axis=1)[0])\n",
    "            preds = np.array(preds)\n",
    "\n",
    "        # So s?nh d? ?o?n v?i nh?n th?t\n",
    "        for label, pred in zip(labels.numpy(), preds):\n",
    "            if label == 0:  # fake\n",
    "                total_fake += 1\n",
    "                if pred != 0:\n",
    "                    misclassified_fake += 1\n",
    "            elif label == 1:  # real\n",
    "                total_real += 1\n",
    "                if pred != 1:\n",
    "                    misclassified_real += 1\n",
    "\n",
    "    apcer = misclassified_fake / total_fake if total_fake > 0 else 0\n",
    "    bpcer = misclassified_real / total_real if total_real > 0 else 0\n",
    "    acer = (apcer + bpcer) / 2\n",
    "    return acer, apcer, bpcer\n",
    "\n",
    "# ???ng d?n m? h?nh TFLite float (chuy?n ??i t? ONNX sang TFLite float, kh?ng quantized)\n",
    "float16_model_path = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Model\\face_antispoofing_fp16.tflite\"\n",
    "interpreter = tf.lite.Interpreter(model_path=float16_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# T?nh to?n c?c ch? s?\n",
    "acer, apcer, bpcer = calculate_metrics_float16(interpreter, test_loader)\n",
    "print(f\"ACER: {acer:.4f}\")\n",
    "print(f\"APCER: {apcer:.4f}\")\n",
    "print(f\"BPCER: {bpcer:.4f}\")"
   ],
   "id": "a3131b29b576f973",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACER: 0.0200\n",
      "APCER: 0.0112\n",
      "BPCER: 0.0288\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T03:24:06.772548Z",
     "start_time": "2025-03-11T03:24:06.765720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "def get_model_size(file_path):\n",
    "    size_bytes = os.path.getsize(file_path)\n",
    "    size_mb = size_bytes / (1024 * 1024)\n",
    "    return size_mb\n",
    "\n",
    "keras_model = r\"C:\\Users\\GIGABYTE\\PycharmProjects\\Paper-FAS\\Model\\face_antispoofing_fp16.tflite\"\n",
    "size = get_model_size(keras_model)\n",
    "print(f\"K?ch th??c m? h?nh: {size:.2f} MB\")\n"
   ],
   "id": "a5aec9b48ed97baa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K?ch th??c m? h?nh: 11.76 MB\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3afb68bbd984d74"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
